
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Lesson 04 - Supervised Machine Learning - Support Vector Machines &#8212; HEADS LEARN v0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=bd9e20870c6007c4c509" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=bd9e20870c6007c4c509" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=bd9e20870c6007c4c509"></script>

    <script src="_static/documentation_options.js?v=d3792fb7"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lesson04';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Lesson 05 - Supervised Machine Learning - Artificial Neural Networks" href="lesson05.html" />
    <link rel="prev" title="Lesson 03 - Graphical Models - Bayesian Networks" href="lesson03.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <header>
  
    <div class="bd-header navbar navbar-expand-lg bd-navbar">
    </div>
  
  </header>

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">HEADS LEARN v0.1 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lesson01.html">Lesson 01 - Your First ML project in R</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson02.html">Lesson 02 - Supervised Machine Learning - Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson03.html">Lesson 03 - Graphical Models - Bayesian Networks</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Lesson 04 - Supervised Machine Learning - Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson05.html">Lesson 05 - Supervised Machine Learning - Artificial Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson06.html">Lesson 06 - Ensemble Models - Bagging, Boosting and Reliability</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson07.html">Lesson 07 - Unsupervised Machine Learning - Clustering and Association Rules</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson08.html">Lesson 08 - Unsupervised Machine Learning - Data Preprocessing and Anomaly Detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson09.html">Lesson 09 - Machine Learning from Data Streams</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson10.html">Lesson 10 - Visual Data Mining</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson11.html">Lesson 11 - Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson12.html">Lesson 12 - Text Mining</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/lesson04.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lesson 04 - Supervised Machine Learning - Support Vector Machines</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-classification">Linear Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#perpectrons">Perpectrons</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimal-separating-hyperplanes">Optimal Separating Hyperplanes</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#support-vector-classifier">Support Vector Classifier</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="lesson-04-supervised-machine-learning-support-vector-machines">
<h1>Lesson 04 - Supervised Machine Learning - Support Vector Machines<a class="headerlink" href="#lesson-04-supervised-machine-learning-support-vector-machines" title="Link to this heading">#</a></h1>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<section id="linear-classification">
<h3>Linear Classification<a class="headerlink" href="#linear-classification" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>We can always divide the input space into a collection of regions labelled according to the classification.</p></li>
<li><p>The boundaries of these regions can be rough or smooth, depending on the prediction function.</p></li>
<li><p>For an important class of procedures, these decision boundaries are linear; this is what we will mean by linear methods for classification.</p></li>
</ul>
<p>“Fit linear regression models to the class indicator variables, and classify to the
largest fit. The decision boundary between two classes is that set of points for
which prediction done by the two linear models is the same.” Hastie, Tibshirani, Friedman (2001)</p>
<p>“Model discriminant functions for each class, and then classify points to the class
with the largest value for its discriminant function.” Hastie, Tibshirani, Friedman (2001)</p>
<ul class="simple">
<li><p>Any linear monotone transformation of the discriminant function will create linear
decision boundaries.</p></li>
<li><p>For example, using the logit function – log[p/(1-p)] - the decision boundary is the set of points for which the log-odds are zero.</p>
<ul>
<li><p>Methods that use logit:</p>
<ul>
<li><p>Linear discriminant analysis (LDA)</p></li>
<li><p>Linear logistic regression</p></li>
</ul>
</li>
</ul>
</li>
<li><p>A more direct approach is to explicitly model the boundaries between the classes as linear.</p></li>
<li><p>For a two-class problem in a p-dimensional input space, this amounts to modelling the decision boundary as a hyperplane.</p></li>
<li><p>Methods include:</p>
<ul>
<li><p>Perceptron (Rosenblatt, 1958)</p></li>
<li><p>Optimal Separating Hyperplane (Vapnik, 1996)</p></li>
</ul>
</li>
</ul>
<p><strong>Linear Discriminants</strong>
“Methods that define optimal separating hyperplanes for the case when two classes are linearly separable.” Hastie, Tibshirani, Friedman (2001)</p>
<p><strong>Generalized Linear Discriminants</strong>
“Linear functions in the augmented space map down to functions of higher order in the original space: hence linear decision boundaries expand to higher order decision boundaries.” Hastie, Tibshirani, Friedman (2001)</p>
<p><strong>Augmented Dimension Maps</strong></p>
<p><img alt="alt text" src="_images/image_4.1.png" /></p>
<p><strong>Working over projections</strong>
<img alt="alt text" src="_images/image_4.2.png" /></p>
<p><strong>Linear discriminant analysis and logistic regression</strong>
“Linear discriminant analysis and logistic regression both estimate linear decision boundaries in similar but slightly different ways.” Hastie, Tibshirani, Friedman (2001)</p>
<p><strong>Separating Hyperplane Classifiers</strong>
These procedures construct linear decision boundaries that explicitly try to separate the data into different classes as well as possible. Hastie, Tibshirani, Friedman (2001)</p>
<p><img alt="alt text" src="_images/image_4.3.png" /></p>
<ul class="simple">
<li><p>Included in the figure (blue lines) are two of the infinitely many possible separating hyperplanes.</p></li>
<li><p>The orange line is the least squares solution to the problem, which does not do a perfect job in separating the points, and makes one error.</p></li>
<li><p>This is the same boundary found by LDA, in light of its equivalence with linear regression in the two-class case.</p></li>
</ul>
<section id="perpectrons">
<h4>Perpectrons<a class="headerlink" href="#perpectrons" title="Link to this heading">#</a></h4>
<p>“Classifiers such as the one resulting in the orange line, that compute a linear combination of the input features and return the sign of the response, where the line is given by { x : b0 + b1x1 + b2x2 = 0 }.” Rosenblatt (1958)</p>
<p><strong>Perceptron Learning Algorithm</strong>
“The perceptron learning algorithm tries to find a separating hyperplane by minimizing the distance of misclassified points to the decision boundary.” Rosenblatt (1958)</p>
<ul class="simple">
<li><p>The goal is to minimize the error of misclassified data points, which is proportional to the distance of the misclassified points to the decision boundary.</p></li>
<li><p>The algorithm in fact uses stochastic gradient descent to minimize error.</p></li>
<li><p>That is, instead of computing the sum of the (error-based) gradient contributions of each observation followed by a step in the negative gradient direction, a step is taken after each observation is visited.</p></li>
<li><p>Hence the misclassified observations are visited in some sequence, and the parameters are updated via a multiplicative learning rate</p></li>
<li><p><strong>Gradient Descent</strong></p>
<ul>
<li><p>If the classes are linearly separable, it can be shown that the algorithm converges to a separating hyperplane in a finite number of steps. However…</p>
<ul>
<li><p>Problems:</p>
<ul>
<li><p>When the data are separable, there are many solutions, and which one is found depends on the starting values.</p></li>
<li><p>The “finite” number of steps can be very large.</p></li>
<li><p>When the data are not separable, the algorithm will not converge, and cycles develop; the cycles can be long and therefore hard to detect.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img alt="alt text" src="_images/image_4.4.png" /></p>
</section>
<section id="optimal-separating-hyperplanes">
<h4>Optimal Separating Hyperplanes<a class="headerlink" href="#optimal-separating-hyperplanes" title="Link to this heading">#</a></h4>
<p>“The optimal separating hyperplane separates the two classes and maximizes the distance to the closest point from either class” Vapnik (1996)</p>
<ul class="simple">
<li><p>Provides a unique solution to the separating hyperplane problem.</p></li>
<li><p>By maximizing the margin between the two classes on the training data, this leads to better classification performance on test data.</p></li>
</ul>
<p><img alt="alt text" src="_images/image_4.5.png" /></p>
<ul class="simple">
<li><p>The set of conditions of the optimization problem ensure that all the points are at least a signed distance M from the decision boundary defined by β and β0, and we seek the largest such M and associated parameters.</p></li>
</ul>
<p><img alt="alt text" src="_images/image_4.6.png" /></p>
<ul class="simple">
<li><p>Although none of the training observations fall in the margin (by construction), this will not necessarily be the case for test observations.</p></li>
<li><p>The intuition is that a large margin on the training data will lead to good separation on the test data.</p></li>
<li><p>The description of the solution in terms of support points seems to suggest that the optimal hyperplane focuses more on the points that count, and is more robust to model misspecification.</p></li>
<li><p>The LDA solution, on the other hand, depends on all of the data, even points far away from the decision boundary. Note, however, that the identification of these support points required the use of all the data.</p></li>
<li><p>Of course, if the classes are really Gaussian, then LDA is optimal, and separating hyperplanes will pay a price for focusing on the (noisier) data at the boundaries of the classes.</p></li>
<li><p>When a separating hyperplane exists, logistic regression will always find it, since the log-likelihood can be driven to 0 in this case.</p></li>
<li><p>When the data are not separable, there will be no feasible solution to this problem, and an alternative formulation is needed.</p></li>
<li><p>Again one can enlarge the space using basis transformations, but this can lead to artificial separation through over-fitting.</p></li>
</ul>
<p><strong>Separable and non-separable classification problems</strong></p>
<p><img alt="alt text" src="_images/image_4.7.png" /></p>
<p>Suppose now that the classes overlap in feature space.
One way to deal with the overlap is to still maximize M, but allow for some points to be on the wrong side of the margin.</p>
</section>
<section id="support-vector-classifier">
<h4>Support Vector Classifier<a class="headerlink" href="#support-vector-classifier" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>The tuning parameter of this procedure is the cost parameter C.</p>
<ul>
<li><p>Points on the wrong side of the boundary are support vectors.</p></li>
<li><p>Points on the correct side of the boundary but close to it (in the margin), are also support vectors.</p></li>
<li><p>Larger values of C focus attention more on (correctly classified) points near the decision boundary, while smaller values involve data further away.</p></li>
<li><p>Either way, misclassified points are given weight, no matter how far away</p></li>
</ul>
</li>
</ul>
<p><img alt="alt text" src="_images/image_4.8.png" /></p>
<ul class="simple">
<li><p>The margin is larger for C = 0.01 (85% points are support vectors) than it is for C = 10,000 (62% of points are support vectors).</p></li>
<li><p>In this example the procedure is not very sensitive to choices of C, because of the rigidity of a linear boundary.</p></li>
</ul>
<p>“(A support vector machine) Produces nonlinear boundaries by constructing a linear boundary in a large, transformed version of the feature space.” Vapnik (1996)</p>
<p>As with other linear methods, we can make the procedure more flexible by enlarging the feature space using basis expansions such as polynomials or splines.
Generally linear boundaries in the enlarged space achieve better training-class separation, and translate to nonlinear boundaries in the original space, using basis functions
<img alt="alt text" src="_images/image_4.9.png" />
the classifier function is then given by the sign of
<img alt="alt text" src="_images/image_4.10.png" />
The support vector machine classifier is an extension of this idea, where the dimension of the enlarged space is allowed to get very large, infinite in some cases.</p>
<p>It might seem that the computations would become prohibitive.
In fact, due to the representation of the optimization problem involving only inner products, we need not specify the transformations h(x) at all, but require only knowledge of the kernel function
<img alt="alt text" src="_images/image_4.11.png" />
that computes inner products in the transformed space.
Three popular kernels:
<img alt="alt text" src="_images/image_4.12.png" /></p>
<p><img alt="alt text" src="_images/image_4.13.png" /></p>
<p>In each case C was tuned to approximately achieve the best test error performance, and C = 1 worked well in both cases. The radial basis kernel performs the best (close to Bayes optimal), as might be expected given the data arise from mixtures of Gaussians. The broken purple curve in the background is the Bayes decision boundary.</p>
</section>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="lesson03.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Lesson 03 - Graphical Models - Bayesian Networks</p>
      </div>
    </a>
    <a class="right-next"
       href="lesson05.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Lesson 05 - Supervised Machine Learning - Artificial Neural Networks</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-classification">Linear Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#perpectrons">Perpectrons</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimal-separating-hyperplanes">Optimal Separating Hyperplanes</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#support-vector-classifier">Support Vector Classifier</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Mariana Canelas-Pais
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024, Mariana Canelas-Pais.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=bd9e20870c6007c4c509"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=bd9e20870c6007c4c509"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>